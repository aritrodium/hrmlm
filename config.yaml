
## 3. `config.yaml`
```yaml
# Model Configuration
model:
  architecture: "hrmlm"
  vocab_size: 32000  # SentencePiece vocabulary size
  n_ctx: 1024        # Context window size
  n_embd: 768        # Embedding dimension
  n_hidden: 1024     # Hidden dimension
  n_high_hidden: 768 # High-level hidden dimension
  T: 5               # Low-level cycles per step
  N: 3               # High-level cycles per token
  dropout: 0.1       # Dropout rate
  layer_norm: true   # Use layer normalization
  
# Tokenizer Configuration
tokenizer:
  model_type: "bpe"
  character_coverage: 1.0
  max_sentence_length: 16384
  add_dummy_prefix: false
  remove_extra_whitespaces: true
  
# Dataset Configuration
dataset:
  train_file: "data/processed/train.txt"
  val_file: "data/processed/val.txt"
  test_file: "data/processed/test.txt"
  sequence_length: 1024
  batch_size: 32
  num_workers: 4
  pin_memory: true
  persistent_workers: true
  
# Training Configuration
training:
  epochs: 10
  learning_rate: 6e-4
  betas: [0.9, 0.95]
  weight_decay: 0.1
  grad_clip: 1.0
  warmup_steps: 2000
  lr_scheduler: "cosine"
  
# Optimization
optimization:
  gradient_accumulation_steps: 4
  mixed_precision: "fp16"  # "no", "fp16", "bf16"
  compile_model: false      # Use torch.compile
  
# Checkpointing
checkpoint:
  save_dir: "checkpoints"
  save_every: 1000
  keep_last: 5
  log_dir: "logs"
  
# Generation
generation:
  temperature: 0.8
  top_k: 50
  top_p: 0.95
  max_length: 512
