# DPO Configuration for HRMLM

# Model Configuration
model:
  n_ctx: 1024
  n_embd: 768
  n_hidden: 1024
  n_high_hidden: 768
  T: 5
  N: 3
  dropout: 0.1
  layer_norm: true

# Tokenizer Configuration
tokenizer:
  model_path: "spm.model"

# Dataset Configuration
dataset:
  train_path: "data/alpaca_dpo_train.json"  # DPO formatted data
  val_path: "data/alpaca_dpo_val.json"
  max_length: 1024
  num_train_examples: null
  num_val_examples: 500

# DPO Configuration
dpo:
  sft_model_path: "checkpoints/sft/sft_best.pth"  # SFT model to start from
  beta: 0.1  # DPO temperature parameter
  loss_type: "sigmoid"  # "sigmoid" or "hinge"

# Training Configuration
training:
  epochs: 5  # DPO usually converges faster
  batch_size: 2  # Smaller batch size due to two forward passes
  eval_batch_size: 4
  learning_rate: 1e-6  # Smaller learning rate for DPO
  betas: [0.9, 0.95]
  weight_decay: 0.01
  grad_clip: 1.0
  gradient_accumulation_steps: 8  # Larger due to small batch size
  warmup_steps: 50
  min_lr: 1e-7
  scheduler: "cosine"
  use_cpu: false

# Checkpoint Configuration
checkpoint:
  save_dir: "checkpoints/dpo"
  keep_last: 3

# Logging Configuration
logging:
  use_wandb: false
  wandb_project: "hrmlm-dpo"
  run_name: "hrmlm-alpaca-dpo"
