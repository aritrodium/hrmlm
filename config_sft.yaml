# SFT Configuration for HRMLM

# Model Configuration
model:
  n_ctx: 1024
  n_embd: 768
  n_hidden: 1024
  n_high_hidden: 768
  T: 5
  N: 3
  dropout: 0.1
  layer_norm: true

# Tokenizer Configuration
tokenizer:
  model_path: "spm.model"

# Dataset Configuration
dataset:
  train_path: "data/alpaca_train.json"
  val_path: "data/alpaca_val.json"
  max_length: 1024
  num_train_examples: null  # null = use all
  num_val_examples: 1000    # smaller for faster evaluation

# Training Configuration
training:
  pretrained_path: "checkpoints/pytorch_model_epoch4.bin"  # from pretraining
  epochs: 10
  batch_size: 4
  eval_batch_size: 8
  learning_rate: 2e-5
  betas: [0.9, 0.95]
  weight_decay: 0.1
  grad_clip: 1.0
  gradient_accumulation_steps: 4
  warmup_steps: 100
  min_lr: 1e-6
  scheduler: "cosine"
  use_cpu: false  # force CPU even if GPU available

# Checkpoint Configuration
checkpoint:
  save_dir: "checkpoints/sft"
  keep_last: 3

# Generation Configuration (for evaluation)
generation:
  max_length: 200
  temperature: 0.7
  top_k: 50
  top_p: 0.9
  example_freq: 2  # generate examples every N epochs

# Logging Configuration
logging:
  use_wandb: false
  wandb_project: "hrmlm-sft"
  run_name: "hrmlm-alpaca-sft"
